---
title: A survey of  the quality and transparency of statistical power analyses conducted
  using GPower
author: |
  |
  | Robert T. Thibault & Emmanuel Zavalis
  | (Contributor roles are detailed before the references)
  |
  | Address correspondence to robert.thibault@stanford.edu
date: "`r Sys.Date()`"
output:
  pdf_document:
    includes:
      in_header: my_header.tex
  word_document: default
header-includes: \usepackage{caption}
---
\captionsetup[table]{labelformat=empty}


```{r packages, echo=FALSE, include=FALSE}
library(readr)
library(tidyverse) # for cleaner code
library(knitr) # for knitting and kable function
library(kableExtra) # for kable table styling
options(scipen = 100)
```

```{r setup, echo=FALSE}

# go in and delete the first row from the gpower form, and the third row that are meta-data and irrelevant to our dataframe
# remove every column up to reviewer initial (since IP and exact location is revealed :-) )
df_gpower_orig=read.csv('./gpower_pilotData.csv')
df_gpower_orig=df_gpower_orig[which(df_gpower_orig$reviewer=='EZ'),]

#remove the ineligibile
df_gpower=df_gpower_orig[which(df_gpower_orig$include=='Include'),]


# check
dfs=read.csv('pmc_result.txt',col.names='PMCID')
lista=as.list(dfs$PMCID)
set.seed(1453)
samples=sample(lista,15)

searchstring=paste(unlist(samples),collapse=" OR ")
pmidsearch<-file("pmcidsearch_general.txt")
writeLines(searchstring, pmidsearch)
close(pmidsearch)

p = 0.5
moe = 0.2
alpha=0.05
z=qnorm(p=alpha, lower.tail=TRUE)*-1

```

```{r extrapolate,echo=FALSE}
# this chunk estimates the total number of published articles that use GPower for doing (1) a power analysis, or (2) an a priori sample size calculation

#fileEncoding variable so that the column headers don't have junk text
ex=read.csv('./queryHits.csv', header=T, fileEncoding="UTF-8-BOM")

# number of articles coded
denom <- nrow(df_gpower_orig)
# number of articles meeting our inclusion criteria
include <- nrow(df_gpower)
# number of articles used for a priori sample size calculations
apriori <- sum(grepl("A priori", df_gpower$typeof_powercalc) & grepl("Before", df_gpower$when_powercalc))
# percentage of PMC articles that were hits
proportion <- ex$pmcHits/ex$pmcTotal

#initialize dataframe for point estimates and confidence intervals
cis <- data.frame(matrix(nrow=6, ncol=3))

# create function to calculate the point estimates and confidence intervals
ciCalc <- function(raw, proportion, num, denom){
  cis <- c(raw * proportion * num / denom,
           raw * proportion * prop.test(x=num, n=denom, conf.level=.95)$conf.int[1],
           raw * proportion * prop.test(x=num, n=denom, conf.level=.95)$conf.int[2]
  )
  return(cis)
}

# run the function to calculate point estimates and confidence intervals 
cis <- rbind(ciCalc(ex$pmcTotal, proportion, include, denom),
            ciCalc(ex$pmcTotal, proportion, apriori, denom),
            ciCalc(ex$pubmedTotal, proportion, include, denom),
            ciCalc(ex$pubmedTotal, proportion, apriori, denom),
            ciCalc(ex$dimensionsTotal, proportion, include, denom),
            ciCalc(ex$dimensionsTotal, proportion, apriori, denom)
)

# assign row names and column names
rownames(cis) <- c("pmcInclude", 
                    "pmcApriori",
                    "pubmedInclude", 
                    "pubmedApriori",
                    "dimensionsInclude", 
                    "dimensionsApriori"
                    )
colnames(cis) <- c("point", 
                    "ci.lb",
                    "ci.ub" 
                    )        


cis <- cis %>% round(0)

#supplementary table 1
st2 <- cis
rownames(st2) <- c("PubMed Central included", 
                    "PubMed Central a priori",
                    "PubMed included", 
                    "PubMed, a priori",
                    "Dimensions included", 
                    "Dimensions a priori"
                    )
colnames(st2) <- c("Point estimate", 
                    "95% confidence Interval, lower bound",
                    "95% confidence Interval, upper bound"
                    )  

minapriori=st2[4,2]
maxapriori=st2[6,3]

minanypowercalc=st2[3,2]
maxanypowercalc=st2[5,3]

```

```{r, suppTable1, include = TRUE, echo = FALSE}
timingdf=table(df_gpower$typeof_powercalc, df_gpower$when_powercalc)

supptable1=knitr::kable(timingdf, caption = "Supplementary Table 1. Type and timing of power calculations", booktabs = T, linesep = "", align = "c") %>% 
  kable_styling(latex_options = "striped")%>%kable_styling(latex_options = c("hold_position"))
```

```{r magic megatable data, echo=FALSE, include=FALSE}
df_gpower$stat_test=''
df_gpower$error=''

# Functions to make this a little easier

rounder=function(x){
  if (round(x,0)!=100)
  {
   return(round(x,0))}
  else 
  {return(round(x,1))}}

rounder(99.3)

makeCI <- function(num, denom){
  props=prop.test(num,denom)
  lower=as.character(rounder(props[['conf.int']][1]*100))
  upper=as.character(rounder(props[['conf.int']][2]*100))
  CI=paste0(lower,', ',upper)
  estimate =as.character(rounder(props[['estimate']]*100))
  CInestimate=paste0(estimate,' (',CI,')')
  return(CInestimate)
}


makeProp<- function(num, denom){
  props=prop.test(num,denom)
  estimate =as.character(rounder(props[['estimate']]*100))
  return(as.character(estimate))
}


# Alpha
alphareported=makeCI(nrow(df_gpower[which(df_gpower$alpha_reported=='Yes'),]),nrow(df_gpower))
alpha005=makeProp(nrow(df_gpower[which(df_gpower$alpha_value=='0.05'),]),nrow(df_gpower))
alphaother=makeProp(nrow(df_gpower[which(df_gpower$alpha_value!='0.05'),]),nrow(df_gpower))

# Power
powerreported=makeCI(nrow(df_gpower[which(df_gpower$power_reported=='Yes'),]),nrow(df_gpower))
power80=makeProp(nrow(df_gpower[which(df_gpower$power_value==0.8),]),nrow(df_gpower))
power95=makeProp(nrow(df_gpower[which(df_gpower$power_value==0.95),]),nrow(df_gpower))

powerother=makeProp(nrow(df_gpower[which(df_gpower$power_value!=0.8 && df_gpower$power_value!=0.95),]),nrow(df_gpower[which(df_gpower$power_reported=='Yes'),]))

# Effect size
effectsizef=round(nrow(df_gpower[which(df_gpower$typeof_effectsize=='f'),])/nrow(df_gpower)*100,1)
effectsized=round(nrow(df_gpower[which(df_gpower$typeof_effectsize=='d'),])/nrow(df_gpower)*100,1)
effectsizeother=round(nrow(df_gpower[!grepl('f|d',df_gpower$typeof_effectsize),])/nrow(df_gpower)*100,1)
effectsizereported=makeCI(sum(!is.na(df_gpower$typeof_effectsize)),nrow(df_gpower))

# Effect size justification
justifiedoverall=makeCI(nrow(df_gpower[!grepl('No', df_gpower$effectsize_justification),]),nrow(df_gpower))
justifiedpilot=makeProp(nrow(df_gpower[grepl('pilot', df_gpower$effectsize_justification),]),nrow(df_gpower))
justifiedpreviousres=makeProp(nrow(df_gpower[grepl('Previous research', df_gpower$effectsize_justification),]),nrow(df_gpower))

#round(df_gpower[which(df_gpower$typeof_effectsize=='f'),]/nrow(df_gpower)*100,1)
#df_gpower$samplesize_value

# samplesize
samplesizereported=makeCI(sum(!is.na(df_gpower$samplesize_value)),nrow(df_gpower))
samplesizemedian=median(as.numeric(df_gpower[which(!is.na(df_gpower$samplesize_value)),]$samplesize_value))
samplesizeiqr=IQR(as.numeric(df_gpower[which(!is.na(df_gpower$samplesize_value)),]$samplesize_value))
samplesizenumber=paste0(as.character(samplesizemedian),' (', as.character(samplesizeiqr), ')')

# type of test
stattest='' # dummy variable
stattest=makeCI(sum(!is.na(df_gpower$typeof_effectsize)), nrow(df_gpower))
stattestt=round(nrow(df_gpower[grep('T-test',df_gpower$stat_test),])/nrow(df_gpower)*100,2)
stattestanova=round(nrow(df_gpower[grep('ANOVA', df_gpower$stat_test),])/nrow(df_gpower)*100,2)
stattestother=round(nrow(df_gpower[!grepl('T-test|ANOVA|NA',df_gpower$stat_test),])/nrow(df_gpower)*100,2)

#reproducible?
reproducibleoverall=makeCI(nrow(df_gpower[grep('Yes', df_gpower$powercalc_reproducible),]),nrow(df_gpower))
reproduciblewassumptions=makeProp(nrow(df_gpower[grep('make some assumptions', df_gpower$powercalc_reproducible),]),nrow(df_gpower))
reproduciblewoassumptions=makeProp(nrow(df_gpower[grep('Yes, based solely on the information in the article or its supplementary material.', df_gpower$powercalc_reproducible),]),nrow(df_gpower))

# Multiple comparisons
multiple_comparisons=makeProp(nrow(df_gpower[grep('Yes',df_gpower$multiple_comparisons_powercalc),]), (nrow(df_gpower)-nrow(df_gpower[grep('No, and there is no reason',df_gpower$multiple_comparisons_powercalc),])))


magicmegadf=as.data.frame(rbind(alphareported, alpha005, alphaother,powerreported, power80, power95, powerother, effectsizereported, effectsizef, effectsized, effectsizeother, stattest, stattestanova,stattestt,stattestother,samplesizereported, samplesizenumber, justifiedoverall,justifiedpreviousres,justifiedpilot, reproducibleoverall, reproduciblewassumptions,reproduciblewoassumptions, multiple_comparisons))

rownames(magicmegadf)=c('Alpha (95% CI)','0.05', 'Other level of significance', 'Power (95% CI)', '80%','95%','Other level of power','Effect size (95% CI)', 'd', 'f', 'Other', 'Statistical test (95% CI)', 'ANOVA', 'T-test', 'Other test', 'Sample size reported (95% CI)', 'Sample size median (IQR)', 'Any justification for their effect size (95% CI)', 'Justified: previous research','Justified: own pilot',  'Reproducible (95% CI)','Reproducible with assumptions','Reproducible w/o assumptions', 'Adjusted for multiple comparisons')

```

```{r, magicmegadtable, echo=FALSE}
magicmegadtable=magicmegadf %>%
  kbl(caption = "Table 1. Complete reporting and reproducibility of GPower calculations", col.names=c('% Reporting')) %>%
  kable_styling(latex_options = "striped")%>% kable_styling(latex_options = c("hold_position"))%>%
  #kable_classic(full_width = T, html_font = "Cambria")%>%
  
  # redo that stuff...
  add_indent(c(2, 3,5,6,7,9,10,11,13,14,15,19,20,22,23)) %>%
  add_footnote(c('Note that we have used placeholder variables for the rows statistical tests, these will be replaced with actual data','The sums of percentages may not add up since the numbers are rounded'), notation='symbol')
```


# Methods
## Search method and selection criteria

This is a pilot study that was conducted prior to the extraction of the actual sample that we will study in our paper. Using the search strategy ‘g*power’ in PubMed Central we searched for the papers that used the software GPower for their power analysis to examine their use of this tool and the handling of power calculations in general. The search was conducted on April 10th 2022 and retrieved `r nrow(dfs)` publications. 

From these a random sample of `r length(samples)` articles were examined in detail using random seed 1453. 

The search for the sample to find the articles and was used in PubMed Central was:

'`r paste(samples,collapse=' OR ')`'


We assessed the papers for eligibility first with the inclusion criteria being that the paper uses GPower to perform a power calculation. We also extracted meta-data that included what journal it was from, what impact factor the journal has, as well as the publication year. Finally we looked into whether the paper was a clinical trial defined as a study that includes human subjects and studies an intervention.  
 
# Results
## Search results
We sampled `r nrow(df_gpower_orig)` papers of which `r nrow(df_gpower[which(df_gpower$include=='Include'),])` met our inclusion criteria. Eight of these performed power calculations to solve for sample size and the power calculation appears to be conducted before running the study. One solved for power and one solved for effect size (see Supplementary Table 2 for all counts). 

[Supplementary Table 1]
```{r, echo=FALSE, include=FALSE} 
supptable1
```

Of the articles we surveyed # had human participants, # had non-human animals. # (%) were protocols. 

```{r, echo=FALSE}
error=qt(0.975,df=length(na.omit(df_gpower$impact_factor))-1)*sd(df_gpower$impact_factor, na.rm=TRUE)/sqrt(na.omit(length(df_gpower$impact_factor)))

#row_means <- apply(X=data, MARGIN=1, FUN=mean, na.rm=TRUE)

# fix the impact factor shit to solve so that you can have the mean ignoring nans
impact_factor=paste0(round(mean(df_gpower$impact_factor,na.rm=TRUE),1),' (95% CI ',round(mean(df_gpower$impact_factor,na.rm=TRUE)-error,1), ', ',round(mean(df_gpower$impact_factor,na.rm=TRUE)+error,1),')' )

```


### Meta-data of the articles
The impact factors of the journals the studied papers was on average `r impact_factor`. The papers were published from `r min(df_gpower$publication_year,na.rm=TRUE)` to `r max(df_gpower$publication_year,na.rm=TRUE)`. 

### Estimation of amount of articles affected
We estimate that between `r minapriori` and `r maxapriori` published since 2017 use GPower for an a priori sample size calculation and that between `r minanypowercalc` and `r maxanypowercalc` since 2017 do so for any power calculation (see Supplementary Table 2 for additional details)

[Supplementary Table 2]
```{r, suppTable2, include = TRUE, echo = FALSE, include=FALSE}
supptable2=knitr::kable(st2, caption = "Supplementary Table 2. Estimates of the number of published articles that use GPower.", booktabs = T, linesep = "", align = "c") %>% 
  kable_styling(latex_options = "striped")%>%kable_styling(latex_options = c("hold_position")) %>%
  kable_styling(font_size = 8) %>%
    add_footnote(paste0("The total number of article in each database since 2017 is: PubMed Central ", prettyNum(ex$pmcTotal, big.mark=",", scientific=F), "; PubMed ", prettyNum(ex$pubmedTotal, big.mark=",", scientific=F), "; dimensions.ai ", prettyNum(ex$dimensionsTotal, big.mark=",", scientific=F), "."), notation = "symbol", threeparttable = T)

```


## Reproducibility of power calculations
Articles often did not report key values necessary to reproduce power analyses, including alpha, power, effect size, sample size, and statistical test (Table 1). We could only reproduce `r nrow(df_gpower[grep('Yes',df_gpower$powercalc_reproducible),])` of the power calculations and  `r nrow(df_gpower[grep('assumptions',df_gpower$powercalc_reproducible),])` of them required that we make assumptions.

[Table 1]

An aspect of error types and their rate will be brought up in the study and included in Table 2.

[Table 2]

\newpage


# Tables

```{r, echo=FALSE} 
magicmegadtable
```



\newpage

## Table 2. Error in GPower calculations

[errors in the GPower calculations will be tabulated into a table of the errors manually and inserted into the finished manuscript]

\newpage

```{r, echo=FALSE} 
supptable1
```

\newpage

```{r, echo=FALSE} 
supptable2
```

\newpage

## Further supplementals of summary data for questions not fully presented in the main text (including justification,  multiple comparisons, ANOVA, match, additional error)

```{r, echo=FALSE} 
i=knitr::kable(table(df_gpower$effectsize_justification), col.names=c('Type of justification','No. of articles'), caption='Supplementary Table 3. Justification of effect size') %>%kable_styling(latex_options = "striped")%>%kable_styling(latex_options = c("hold_position"))

i
```

\newpage
```{r, echo=FALSE} 
i=knitr::kable(table(df_gpower$multiple_comparisons_powercalc), col.names=c('Does it adjust for multiple comparisons?','No. of articles'), caption='Supplementary Table 4. Adjustment for multiple comparisons') %>%kable_styling(latex_options = "striped")
i=i%>%column_spec(1, width = "13em")

i
```

\newpage

```{r, echo=FALSE} 
i=knitr::kable(table(df_gpower$ANOVA_within_between.), col.names=c('Does it adjust for the ANOVA conundrum?','No. of articles'), caption='Supplementary Table 5. The ANOVA conundrum') %>%kable_styling(latex_options = "striped")
                                                                                          i=i%>%column_spec(1, width = "13em")
                                                                                          i
```


\newpage

```{r, echo=FALSE} 
i=knitr::kable(table(df_gpower$matching_powercalc_stats), col.names=c('Does the power calculation match the statistical test?','No. of articles'), caption = 'Supplementary Table 6. The match of statistical analysis to power analyses ') %>%kable_styling(latex_options = "striped")

i=i%>%column_spec(1, width = "13em")

i
```

